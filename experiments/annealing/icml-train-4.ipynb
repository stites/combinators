{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:/cloud/syncthing/git/combinators appended to python path\n",
      "INFO:root:%load_ext autoreload\n",
      "INFO:root:%autoreload 2\n",
      "INFO:root:from IPython.core.debugger import set_trace\n",
      "INFO:root:from IPython.core.display import display, HTML\n",
      "INFO:root:import torch\n",
      "INFO:root:import numpy as np\n",
      "INFO:root:import scipy as sp\n",
      "INFO:root:import matplotlib\n",
      "INFO:root:import matplotlib.pyplot as plt\n",
      "INFO:root:%matplotlib inline\n",
      "INFO:root:import seaborn as sns\n",
      "INFO:root:import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "%run ../startup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From test_annealing_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import math\n",
    "from torch import nn, Tensor, optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import trange\n",
    "from typing import Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "from pytest import mark, fixture\n",
    "\n",
    "import combinators.trace.utils as trace_utils\n",
    "from combinators.trace.utils import RequiresGrad\n",
    "from combinators.tensor.utils import autodevice, kw_autodevice, copy, show\n",
    "from combinators.densities import MultivariateNormal, Tempered, RingGMM, Normal\n",
    "from combinators.densities.kernels import MultivariateNormalKernel, MultivariateNormalLinearKernel, NormalLinearKernel\n",
    "from combinators.nnets import ResMLPJ\n",
    "from combinators.objectives import nvo_rkl, nvo_avo, mb0, mb1, _estimate_mc, eval_nrep, _nvo_rkl\n",
    "from combinators import Forward, Reverse, Propose, Condition, RequiresGrad, Resample\n",
    "from combinators.stochastic import RandomVariable, ImproperRandomVariable\n",
    "from combinators.metrics import effective_sample_size, log_Z_hat\n",
    "from tests.utils import is_smoketest, seed\n",
    "import combinators.debug as debug\n",
    "\n",
    "import experiments.annealing.visualize as V\n",
    "from experiments.annealing.models import mk_model, sample_along, paper_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-APG additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from combinators.utils import load_models, save_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_annealing_experiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(writer, ess, lzh, loss_scalar, i, eval_break, targets, forwards, saveable_models, comment):\n",
    "    with torch.no_grad():\n",
    "        # loss\n",
    "        writer.add_scalar('loss', loss_scalar, i)\n",
    "\n",
    "        # ESS\n",
    "        for step, x in zip(range(1,len(ess)+1), ess):\n",
    "            writer.add_scalar(f'ess/step-{step}', x, i)\n",
    "\n",
    "        # logZhat\n",
    "        for step, x in zip(range(1,len(lzh)+1), lzh):\n",
    "            writer.add_scalar(f'log_Z_hat/step-{step}', x, i)\n",
    "\n",
    "        # show samples\n",
    "        if i % eval_break == 0:\n",
    "            samples = sample_along(targets[0], forwards)\n",
    "            fig = V.scatter_along(samples)\n",
    "            writer.add_figure('overview', fig, global_step=i, close=True)\n",
    "\n",
    "            # =================================================================================================================================\n",
    "            # POST-APG additions\n",
    "            # =================================================================================================================================\n",
    "            save_models(saveable_models, filename=comment)\n",
    "            # =================================================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_runner(is_smoketest, trainer, resample, objective_tpl, device, budget, num_iterations, num_targets=6, lr=1e-3, name=\"\"):\n",
    "    debug.seed()\n",
    "    eval_break=50\n",
    "    num_iterations=3 if is_smoketest else num_iterations\n",
    "    # Setup\n",
    "    oname = objective_tpl[0]\n",
    "    objective = objective_tpl[1]\n",
    "\n",
    "    # Models\n",
    "    out = paper_model(num_targets=num_targets, **kw_autodevice(device))\n",
    "    num_samples = budget // len(out['targets'])\n",
    "    sample_shape=(num_samples,)\n",
    "    comment=f\"icml-annealing-{name}_{oname}-{len(out['targets'])}-{'r' if resample else '_'}-d{device}-s{num_samples}-i{num_iterations}-lr{lr}\"\n",
    "\n",
    "\n",
    "    targets, forwards, reverses = [[m.to(autodevice()) for m in out[n]] for n in ['targets', 'forwards', 'reverses']]\n",
    "    saveable_models = {f'{k}-{i}': m for k, ms in dict(targets=targets, forwards=forwards, reverses=reverses).items() for i, m in enumerate(ms)}\n",
    "    try:\n",
    "        saveable_models = load_models(saveable_models, comment)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    assert all([len(list(k.parameters())) >  0 for k in [*forwards, *reverses]])\n",
    "\n",
    "    # logging\n",
    "    writer = SummaryWriter(comment=comment)\n",
    "    loss_ct, loss_sum, loss_avgs, loss_all = 0, 0.0, [], []\n",
    "\n",
    "    optimizer = optim.Adam([dict(params=x.parameters()) for x in [*forwards, *reverses]], lr=lr)\n",
    "\n",
    "    with trange(num_iterations) as bar:\n",
    "        for i in bar:\n",
    "\n",
    "            lvss, loss = trainer(i, targets, forwards, reverses, sample_shape, resample, objective, device)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # REPORTING\n",
    "            # ---------------------------------------\n",
    "            with torch.no_grad():\n",
    "                lvs = torch.stack(lvss, dim=0)\n",
    "                lws = torch.cumsum(lvs, dim=0)\n",
    "                ess = effective_sample_size(lws, sample_dims=-1)\n",
    "                lzh = log_Z_hat(lws, sample_dims=-1)\n",
    "\n",
    "                loss_scalar = loss.detach().cpu().mean().item()\n",
    "\n",
    "                report(writer, ess, lzh, loss_scalar, i, eval_break, targets, forwards, saveable_models, comment=comment)\n",
    "\n",
    "                loss_ct += 1\n",
    "                loss_sum += loss_scalar\n",
    "                # Update progress bar\n",
    "                if i % 10 == 0:\n",
    "                    loss_avg = loss_sum / loss_ct\n",
    "                    loss_template = 'loss={: .4f}'.format(loss_avg)\n",
    "                    logZh_template = 'logZhat[-1]={: .4f}'.format(lzh[-1].cpu().item())\n",
    "                    ess_template = 'ess[-1]={: .4f}'.format(ess[-1].cpu().item())\n",
    "                    loss_ct, loss_sum  = 0, 0.0\n",
    "                    bar.set_postfix_str(\"; \".join([loss_template, ess_template, logZh_template]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvi_eager_resample(i, targets, forwards, reverses, sample_shape, resample, objective, device):\n",
    "    q0 = targets[0]\n",
    "    p_prv_tr, _, _ = q0(sample_shape=sample_shape)\n",
    "\n",
    "    loss = torch.zeros(1, **kw_autodevice(device))\n",
    "    lw, lvss = torch.zeros(sample_shape, **kw_autodevice(device)), []\n",
    "\n",
    "    for k, (fwd, rev, q, p) in enumerate(zip(forwards, reverses, targets[:-1], targets[1:])):\n",
    "        q_ext = Forward(fwd, Condition(q, p_prv_tr, requires_grad=RequiresGrad.NO), _step=k)\n",
    "        p_ext = Reverse(p, rev, _step=k)\n",
    "        extend = Propose(target=p_ext, proposal=q_ext, _step=k)\n",
    "        if resample:\n",
    "            extend = Resample(extend)\n",
    "\n",
    "        state = extend(sample_shape=sample_shape, sample_dims=0, _debug=True)\n",
    "        lv = state.weights\n",
    "\n",
    "        p_prv_tr = state.trace\n",
    "\n",
    "        lw += lv\n",
    "        if resample:\n",
    "            ext_prp = state.program\n",
    "            ext_tar = state.program.target\n",
    "        else:\n",
    "            ext_prp = state.proposal\n",
    "            ext_tar = state.target\n",
    "\n",
    "        loss += objective(lw, lv, ext_prp.trace[f'g{k}'], ext_tar.trace[f'g{k+1}'])\n",
    "\n",
    "        lvss.append(lv)\n",
    "\n",
    "    return lvss, loss\n",
    "\n",
    "rkl_obj = ('rkl', nvo_rkl)\n",
    "_rkl_obj = ('NEWrkl', _nvo_rkl)\n",
    "avo_obj = ('avo', lambda lw, lv, g_k, g_kp1: nvo_avo(lv))\n",
    "budget=288\n",
    "num_iterations=20000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 5709/20000 [06:40<17:53, 13.31it/s, loss=-28891.5802; ess[-1]= 14.2814; logZhat[-1]= 1.7870] "
     ]
    }
   ],
   "source": [
    "experiment_runner(\n",
    "    trainer=nvi_eager_resample,\n",
    "    device=\"cpu\", \n",
    "    is_smoketest=False, \n",
    "    resample=False, \n",
    "    budget=budget, \n",
    "    num_iterations=num_iterations, \n",
    "    num_targets=4,\n",
    "    objective_tpl=_rkl_obj,\n",
    "    name='nvi', \n",
    "    lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_runner(\n",
    "    trainer=nvi_eager_resample,\n",
    "    device=\"cpu\", \n",
    "    is_smoketest=False, \n",
    "    resample=False, \n",
    "    budget=budget, \n",
    "    num_iterations=num_iterations, \n",
    "    num_targets=4,\n",
    "    objective_tpl=avo_obj,\n",
    "    name='avo', \n",
    "    lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_model(resample, objective_tpl, device, budget, lr=1e-3,num_iterations=10000, name=\"\"):\n",
    "    num_iterations= num_iterations\n",
    "    # Setup\n",
    "    oname = objective_tpl[0]\n",
    "    objective = objective_tpl[1]\n",
    "\n",
    "    # Models\n",
    "    out = paper_model(num_targets=8, **kw_autodevice(device))\n",
    "    num_samples =  budget // len(out['targets'])\n",
    "    sample_shape=(num_samples,)\n",
    "    comment=f\"icml-annealing-{name}_{oname}-{'r' if resample else '_'}-d{device}-s{num_samples}-i{num_iterations}-lr{lr}\"\n",
    "\n",
    "    # Models\n",
    "    out = paper_model(**kw_autodevice(device))\n",
    "\n",
    "    targets, forwards, reverses = [[m.to(autodevice()) for m in out[n]] for n in ['targets', 'forwards', 'reverses']]\n",
    "    saveable_models = {f'{k}-{i}': m for k, ms in dict(targets=targets, forwards=forwards, reverses=reverses).items() for i, m in enumerate(ms)}\n",
    "    \n",
    "    saveable_models = load_models(saveable_models, comment)\n",
    "\n",
    "    return targets, forwards, reverses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "budget=288\n",
    "num_iterations=20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "budget=288\n",
    "num_iterations=20000\n",
    "\n",
    "targets, forwards, reverses = forward_model(device=\"cuda\", resample=False, budget=budget, num_iterations=num_iterations, objective_tpl=rkl_obj, name='nvi', lr=1e-3)\n",
    "\n",
    "def weights_along(proposal, kernels, sample_shape=(1000,)):\n",
    "    samples = []\n",
    "    tr, _, out = proposal(sample_shape=sample_shape)\n",
    "    samples.append(out)\n",
    "    for k in kernels:\n",
    "        proposal = Forward(k, proposal)\n",
    "        tr, _, out = proposal(sample_shape=sample_shape)\n",
    "        samples.append((out, tr.log_joint(sample_dims=0, batch_dim=None if len(sample_shape) == 1 else 1)))\n",
    "    return samples\n",
    "\n",
    "\n",
    "def weights_last(proposal, kernels, sample_shape=(1000,)):\n",
    "    for k in kernels:\n",
    "        proposal = Forward(k, proposal)\n",
    "    tr, _, out = proposal(sample_shape=sample_shape)\n",
    "    return out, tr.log_joint(sample_dims=0, batch_dim=None if len(sample_shape) == 1 else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(forwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "count = 100\n",
    "\n",
    "for _ in range(count):\n",
    "    samples = weights_along(targets[0], forwards, sample_shape=(1000,100))[1:]\n",
    "    lvss = [l[1] for l in samples]\n",
    "    lvs = torch.stack(lvss, dim=0)\n",
    "    lws = torch.cumsum(lvs, dim=0)\n",
    "    ess  = effective_sample_size(lws, sample_dims=-1)\n",
    "    tot += ess[-1]\n",
    "\n",
    "print(tot/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = weights_along(targets[0], forwards, sample_shape=(1000,100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvs = torch.stack([l[1] for l in samples[1:]], dim=0)\n",
    "lws = torch.cumsum(lvs, dim=0)\n",
    "\n",
    "effective_sample_size(lws, sample_dims=1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lws = torch.cumsum(lvs, dim=0)\n",
    "ess  = effective_sample_size(lws, sample_dims=-1)\n",
    "tot += ess[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = samples[-1][:,0]\n",
    "y = samples[-1][:,1]\n",
    "samples[-1].shape\n",
    "x, y = samples[-1].T\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_hist(ax, samples, sort=True, bins=20, range=None, weight_cm=False, **kwargs):\n",
    "    ax.tick_params(bottom=False, top=False, left=False, right=False,\n",
    "                   labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n",
    "    ax.grid(False)\n",
    "    #x, y = [sample[:,i].detach().cpu().numpy() for i in [0,1]]\n",
    "    x, y = samples.detach().cpu().numpy().T\n",
    "    mz, x_e, y_e = np.histogram2d(x, y, bins=bins, density=True, range=range)\n",
    "    X, Y = np.meshgrid(x_e, y_e)\n",
    "    if weight_cm:\n",
    "        raise NotImplemented()\n",
    "    else:\n",
    "        ax.imshow(mz, **kwargs)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(3,3))\n",
    "\n",
    "plot_sample_hist(fig.gca(), samples[-1], bins=150, cmap='viridis')\n",
    "# bins = 20\n",
    "# sample= samples[-1]\n",
    "# range=None\n",
    "# x, y = [sample[:,i].detach().cpu().numpy() for i in [0,1]]\n",
    "# mz, x_e, y_e = np.histogram2d(x, y, bins=bins, density=True, range=range)\n",
    "# ax1.imshow(mz, **kwargs)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.image import NonUniformImage\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, y = [sample[:,i].detach().cpu().numpy() for i in [0,1]]\n",
    "\n",
    "H, xedges, yedges = np.histogram2d(x, y, bins=50, density=True)\n",
    "\n",
    "H = H.T  # Let each row list bins with common y range.\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=1, nrows=1)\n",
    "\n",
    "X, Y = np.meshgrid(xedges, yedges)\n",
    "\n",
    "ax1.pcolormesh(X, Y, H)\n",
    "\n",
    "# ax1 = fig.add_subplot(133, title='NonUniformImage: interpolated',\n",
    "#         aspect='equal', xlim=xedges[[0, -1]], ylim=yedges[[0, -1]])\n",
    "\n",
    "# im = NonUniformImage(ax, interpolation='bilinear')\n",
    "\n",
    "# xcenters = (xedges[:-1] + xedges[1:]) / 2\n",
    "\n",
    "# ycenters = (yedges[:-1] + yedges[1:]) / 2\n",
    "\n",
    "# im.set_data(xcenters, ycenters, H)\n",
    "\n",
    "# ax.images.append(im)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
